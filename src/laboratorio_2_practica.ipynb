{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "97e23c3b98feea9f",
      "metadata": {
        "id": "97e23c3b98feea9f"
      },
      "source": [
        "![Machine Learning Lab](banner.jpg)\n",
        "\n",
        "# Laboratorio 2\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. Preparación de datos: Descargar, cargar y preprocesar datasets sobre depresión en estudiantes y precios de laptops, incluyendo imputación de valores faltantes, normalización y codificación de variables.\n",
        "\n",
        "2. Modelado y evaluación.\n",
        "\n",
        "3. Entrenar y evaluar un modelo de regresión logística para predecir depresión en estudiantes.\n",
        "\n",
        "4. Entrenar y evaluar un modelo de regresión lineal para predecir precios de laptops.\n",
        "\n",
        "5. Análisis de overfitting: Identificar overfitting mediante curvas de aprendizaje y comparar el rendimiento en entrenamiento y validación.\n",
        "\n",
        "6. Regularización: Aplicar técnicas de regularización (L1, L2, Elastic Net) en regresión lineal (Ridge, Lasso) y logística para reducir overfitting.\n",
        "\n",
        "7. Optimización de modelos: Usar Grid Search con validación cruzada para encontrar los mejores hiperparámetros y evaluar el modelo optimizado.\n",
        "\n",
        "## Descarga de datasets\n",
        "\n",
        "Antes de comenzar, descarguemos los datasets necesarios para el laboratorio. Estos datasets incluyen información sobre depresión en estudiantes y precios de laptops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77761c75fb12585b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-04T19:47:22.609295Z",
          "start_time": "2025-02-04T19:47:22.471421Z"
        },
        "id": "77761c75fb12585b"
      },
      "outputs": [],
      "source": [
        "!mkdir datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-04T19:47:23.712882Z",
          "start_time": "2025-02-04T19:47:22.617533Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1e2be841-8b82-42a4-f3d7-a8a567911e4f"
      },
      "outputs": [],
      "source": [
        "!curl -L -o datasets/student-depression-dataset.zip https://www.kaggle.com/api/v1/datasets/download/hopesb/student-depression-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1780d12083b0423c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-04T19:51:37.533842Z",
          "start_time": "2025-02-04T19:47:23.993932Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1780d12083b0423c",
        "outputId": "c6925fb5-01fa-400f-95fa-96b16ce2fa9a"
      },
      "outputs": [],
      "source": [
        "!unzip datasets/student-depression-dataset.zip -d datasets/student-depression-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6a134cc682a3b1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-04T19:51:38.770564Z",
          "start_time": "2025-02-04T19:51:37.561701Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b6a134cc682a3b1",
        "outputId": "853f1954-92b3-4d96-eea0-89f0f5032a28"
      },
      "outputs": [],
      "source": [
        "!curl -L -o datasets/laptop-price-dataset.zip https://www.kaggle.com/api/v1/datasets/download/muhammetvarl/laptop-price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2e629650a6924f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-04T19:51:42.056966Z",
          "start_time": "2025-02-04T19:51:38.777266Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df2e629650a6924f",
        "outputId": "6ffef611-d5c1-4dab-8624-b5bc1062134b"
      },
      "outputs": [],
      "source": [
        "!unzip datasets/laptop-price-dataset.zip -d datasets/laptop-price-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe8eb819a476e31",
      "metadata": {
        "id": "dfe8eb819a476e31"
      },
      "source": [
        "## Cargar librerias\n",
        "\n",
        "Importamos las librerías necesarias para el análisis de datos, preprocesamiento, modelado y evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb668a80de60a063",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:54:50.833741Z",
          "start_time": "2025-02-05T14:54:50.670053Z"
        },
        "id": "fb668a80de60a063"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eddd434d0defc8a1",
      "metadata": {
        "id": "eddd434d0defc8a1"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Cargamos los datasets descargados y revisamos su estructura y contenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c782e304f4cdac",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:54:51.564901Z",
          "start_time": "2025-02-05T14:54:51.504066Z"
        },
        "id": "b5c782e304f4cdac"
      },
      "outputs": [],
      "source": [
        "depression_df = pd.read_csv('datasets/student-depression-dataset/Student Depression Dataset.csv')\n",
        "laptop_price_df = pd.read_csv('datasets/laptop-price-dataset/laptop_price.csv', encoding='iso-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abdae9f2bc9c2cc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:54:53.200185Z",
          "start_time": "2025-02-05T14:54:53.186088Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9abdae9f2bc9c2cc",
        "outputId": "1318beb8-12f8-4404-deb6-7448b2e9808f"
      },
      "outputs": [],
      "source": [
        "depression_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f569a6b54ce316",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:54:55.971079Z",
          "start_time": "2025-02-05T14:54:55.963511Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9f569a6b54ce316",
        "outputId": "0639de3b-3cd0-49fb-d74a-74e6abdf9923"
      },
      "outputs": [],
      "source": [
        "laptop_price_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587b227f8dfd3eb6",
      "metadata": {
        "id": "587b227f8dfd3eb6"
      },
      "source": [
        "## 1. Modelos Base sin Regularización\n",
        "\n",
        "### 1.1 Preparación de Datos - Regresión Logística (Depresión)\n",
        "\n",
        "Preparamos los datos para el modelo de regresión logística que predecirá la depresión en estudiantes. Esto incluye la imputación de valores faltantes, la división de datos en conjuntos de entrenamiento y prueba, y la normalización y codificación de variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d85d731b38aef9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:54:58.512931Z",
          "start_time": "2025-02-05T14:54:58.461340Z"
        },
        "id": "79d85d731b38aef9"
      },
      "outputs": [],
      "source": [
        "numeric_features = [\n",
        "    'Age', 'Academic Pressure', 'Work Pressure', 'CGPA',\n",
        "    'Study Satisfaction', 'Job Satisfaction',\n",
        "    'Work/Study Hours', 'Financial Stress'\n",
        "]\n",
        "\n",
        "cat_columns = [\n",
        "    'Gender', 'Dietary Habits',\n",
        "    'Family History of Mental Illness',\n",
        "    'Have you ever had suicidal thoughts ?'\n",
        "]\n",
        "\n",
        "# Dividir datos\n",
        "X_dep = depression_df[numeric_features + cat_columns]\n",
        "y_dep = depression_df['Depression']\n",
        "\n",
        "X_dep_train, X_dep_test, y_dep_train, y_dep_test = train_test_split(\n",
        "    X_dep, y_dep, test_size=0.2, random_state=22\n",
        ")\n",
        "\n",
        "# Pipeline para imputar y escalar valores numéricos\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para imputar y codificar valores categóricos\n",
        "cat_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_pipeline, numeric_features),\n",
        "        ('cat', cat_pipeline, cat_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_dep_train_prepared = preprocessor.fit_transform(X_dep_train)\n",
        "X_dep_test_prepared = preprocessor.transform(X_dep_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79809f5396b3f01",
      "metadata": {
        "id": "e79809f5396b3f01"
      },
      "source": [
        "### 1.2 Regresión Logística Base\n",
        "\n",
        "Entrenamos un modelo de regresión logística sin regularización y evaluamos su rendimiento en los conjuntos de entrenamiento y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dac83a2efacb160",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:01.343078Z",
          "start_time": "2025-02-05T14:55:01.309622Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dac83a2efacb160",
        "outputId": "bddc61f1-7dc1-4e63-edc5-4be6b6234447"
      },
      "outputs": [],
      "source": [
        "# Entrenar modelo\n",
        "log_reg = LogisticRegression(random_state=22)\n",
        "log_reg.fit(X_dep_train_prepared, y_dep_train)\n",
        "\n",
        "# Evaluar modelo\n",
        "y_pred_train_log = log_reg.predict(X_dep_train_prepared)\n",
        "y_pred_test_log = log_reg.predict(X_dep_test_prepared)\n",
        "\n",
        "# Métricas\n",
        "print(\"Métricas Regresión Logística:\")\n",
        "print(\"\\nEntrenamiento:\")\n",
        "print(classification_report(y_dep_train, y_pred_train_log,\n",
        "                            target_names=['No Depression', 'Depression'],\n",
        "                            digits=3))\n",
        "\n",
        "print(\"\\nTest:\")\n",
        "print(classification_report(y_dep_test, y_pred_test_log,\n",
        "                            target_names=['No Depression', 'Depression'],\n",
        "                            digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7f545c9c6d3195",
      "metadata": {
        "id": "4e7f545c9c6d3195"
      },
      "source": [
        "### 1.3 Preparación de Datos - Regresión Lineal (Precios de Laptops)\n",
        "\n",
        "Preparamos los datos para el modelo de regresión lineal que predecirá los precios de las laptops. Similar al anterior, se imputan valores faltantes, se dividen los datos y se normalizan y codifican las variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f2983a20dfc0d6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:03.746446Z",
          "start_time": "2025-02-05T14:55:03.709727Z"
        },
        "id": "c7f2983a20dfc0d6"
      },
      "outputs": [],
      "source": [
        "cat_columns = laptop_price_df.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_features = laptop_price_df.drop(columns=['Price_euros']).select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Dividir datos\n",
        "X_car = laptop_price_df[numeric_features + cat_columns]\n",
        "y_car = laptop_price_df['Price_euros']\n",
        "X_laptop_train, X_laptop_test, y_laptop_train, y_laptop_test = train_test_split(X_car, y_car, test_size=0.2, random_state=22)\n",
        "\n",
        "# Pipeline para imputar y escalar valores numéricos\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para imputar y codificar valores categóricos\n",
        "cat_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    # handle_unknown='ignore' es necesario si en test aparecen categorias que no estaban en train\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_pipeline, numeric_features),\n",
        "        ('cat', cat_pipeline, cat_columns)\n",
        "    ])\n",
        "\n",
        "X_laptop_train_prepared = preprocessor.fit_transform(X_laptop_train)\n",
        "X_laptop_test_prepared = preprocessor.transform(X_laptop_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97736a888f03eee7",
      "metadata": {
        "id": "97736a888f03eee7"
      },
      "source": [
        "### 1.4 Regresión Lineal Base\n",
        "\n",
        "Entrenamos un modelo de regresión lineal sin regularización y evaluamos su rendimiento en los conjuntos de entrenamiento y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bdc886799e3fe6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:05.963594Z",
          "start_time": "2025-02-05T14:55:05.949569Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42bdc886799e3fe6",
        "outputId": "d6af53cb-83c2-4a3c-f40f-370ece578478"
      },
      "outputs": [],
      "source": [
        "# Entrenar modelo\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_laptop_train_prepared, y_laptop_train)\n",
        "\n",
        "# Evaluar modelo\n",
        "y_pred_train_lin = lin_reg.predict(X_laptop_train_prepared)\n",
        "y_pred_test_lin = lin_reg.predict(X_laptop_test_prepared)\n",
        "\n",
        "# Métricas\n",
        "print(\"\\nMétricas Regresión Lineal:\")\n",
        "print(\"\\nEntrenamiento:\")\n",
        "print(f\"MSE: {mean_squared_error(y_laptop_train, y_pred_train_lin):.2f}\")\n",
        "print(f\"R2: {r2_score(y_laptop_train, y_pred_train_lin):.3f}\")\n",
        "\n",
        "print(\"\\nTest:\")\n",
        "print(f\"MSE: {mean_squared_error(y_laptop_test, y_pred_test_lin):.2f}\")\n",
        "print(f\"R2: {r2_score(y_laptop_test, y_pred_test_lin):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d502cdb2814e4abb",
      "metadata": {
        "id": "d502cdb2814e4abb"
      },
      "source": [
        "### 1.5 Overfitting\n",
        "\n",
        "En todos los modelos siempre existe una diferencia entre la calidad de las predicciones en el set de entrenamiento y el set de validación/testing. Esta diferencia se conoce como varianza. Una excesiva varianza se conoce como overfitting.\n",
        "\n",
        "Para entender mejor esto vamos a hacer una gráfica comparando la pérdida entre train y test con distintos tamaños del dataset de entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd591db3a0285426",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:08.418072Z",
          "start_time": "2025-02-05T14:55:08.412678Z"
        },
        "id": "bd591db3a0285426"
      },
      "outputs": [],
      "source": [
        "\n",
        "def linear_regression_learning_curve(model, X, y, plot=True):\n",
        "    # Calcular las curvas de aprendizaje\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator=model,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),  # Porcentajes del dataset de entrenamiento\n",
        "        cv=5,  # Validación cruzada de 5 folds\n",
        "        scoring='neg_mean_squared_error',  # Métrica de evaluación (error cuadrático medio negativo)\n",
        "        n_jobs=-1  # Usar todos los núcleos disponibles\n",
        "    )\n",
        "\n",
        "    # Calcular la media y desviación estándar de los scores\n",
        "    train_scores_mean = -np.mean(train_scores, axis=1)  # Convertir a positivo\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = -np.mean(test_scores, axis=1)  # Convertir a positivo\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    if plot:\n",
        "        # Graficar las curvas de aprendizaje\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Plot training error\n",
        "        sns.lineplot(x=train_sizes, y=train_scores_mean, label='Training error', color='blue', marker='o')\n",
        "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.15, color='blue')\n",
        "\n",
        "        # Plot validation error\n",
        "        sns.lineplot(x=train_sizes, y=test_scores_mean, label='Validation error', color='green', marker='o')\n",
        "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.15, color='green')\n",
        "\n",
        "        # Add titles and labels\n",
        "        plt.title('Learning Curves for Linear Regression', fontsize=16)\n",
        "        plt.xlabel('Training Set Size', fontsize=14)\n",
        "        plt.ylabel('Mean Squared Error', fontsize=14)\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # tomamos la comparacion de un dataset de entrenamiento del 70%\n",
        "    print('Training score (MSE):', train_scores_mean[-3])\n",
        "    print('Test score (MSE):', test_scores_mean[-3])\n",
        "    print('Percent difference, train vs test:', (train_scores_mean[-3] - test_scores_mean[-3]) / test_scores_mean[-3] * 100, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd6fe7ccf75200c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:14.159683Z",
          "start_time": "2025-02-05T14:55:12.189707Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "fbd6fe7ccf75200c",
        "outputId": "c1256b16-5268-42b8-e93c-cd049716dd6d"
      },
      "outputs": [],
      "source": [
        "linear_regression_learning_curve(LinearRegression(), X_laptop_train_prepared, y_laptop_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1557fddf312c6c",
      "metadata": {
        "id": "1b1557fddf312c6c"
      },
      "source": [
        "### Regularización en regresión lineal\n",
        "\n",
        "Como podemos ver en los resultados anteriores la diferencia entre train y validación puede ser amplia.\n",
        "\n",
        "Para resolver esto tenemos estrategias como la regularización esto consiste en penalizar la complejidad del modelo mediante:\n",
        "- `l1` en este caso se penaliza la suma del valor absoluto los parámetros del modelo\n",
        "- `l2` penaliza la suma de los cuadrados de los parámetros de los modelos\n",
        "- `elasticnet` penaliza una combinación lineal de `l1` y `l2`.\n",
        "\n",
        "Cuando una regresión lineal usa `l2` se le conoce como regresión Ridge mientras que cuando usa `l1` se le conoce como regresión Lasso\n",
        "\n",
        "En ambos casos se necesita de un hyper-parámetro alpha para determinar cuánta importancia se le da al MSE y cuánta importancia se le da al MSE\n",
        "\n",
        "`LOSS = MSE + alpha * regularization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee46a227af78943c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:18.043989Z",
          "start_time": "2025-02-05T14:55:17.877645Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "ee46a227af78943c",
        "outputId": "ea088aec-fbd6-4f05-d86b-edf0d3c5f80c"
      },
      "outputs": [],
      "source": [
        "linear_regression_learning_curve(Ridge(alpha=1.0), X_laptop_train_prepared, y_laptop_train, plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf6c8d88b33a7ef",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:52.420069Z",
          "start_time": "2025-02-05T14:55:52.223603Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "1cf6c8d88b33a7ef",
        "outputId": "cadfc663-59e7-4c79-fb06-bc5d6f53faa8"
      },
      "outputs": [],
      "source": [
        "linear_regression_learning_curve(Lasso(alpha=1.0), X_laptop_train_prepared, y_laptop_train, plot=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4be8e6cdcec718",
      "metadata": {
        "id": "8c4be8e6cdcec718"
      },
      "source": [
        "Aquí vemos como la varianza al usar una regresión Lasso es menor que cuando usamos una regresión lineal tradicional.\n",
        "\n",
        "### Regularización en regresión logística\n",
        "\n",
        "En el caso de la regresión logística los mismos argumentos se aplican. Podemos usar regularización para reducir el Overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1989e06daede746",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:54.632033Z",
          "start_time": "2025-02-05T14:55:54.625180Z"
        },
        "id": "d1989e06daede746"
      },
      "outputs": [],
      "source": [
        "def logistic_learning_curve(model, X, y, plot=True):\n",
        "    # Calcular las curvas de aprendizaje\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator=model,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10),  # Porcentajes del dataset de entrenamiento\n",
        "        cv=5,  # Validación cruzada de 5 folds\n",
        "        scoring='f1',  # Métrica de evaluación\n",
        "        n_jobs=-1  # Usar todos los núcleos disponibles\n",
        "    )\n",
        "\n",
        "\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "\n",
        "    if plot:\n",
        "        # Graficar las curvas de aprendizaje\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Plot training score\n",
        "        sns.lineplot(x=train_sizes, y=train_scores_mean, label='Training score', color='blue', marker='o')\n",
        "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.15, color='blue')\n",
        "\n",
        "        # Plot validation score\n",
        "        sns.lineplot(x=train_sizes, y=test_scores_mean, label='Validation score', color='green', marker='o')\n",
        "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.15, color='green')\n",
        "\n",
        "        # Add titles and labels\n",
        "        plt.title(f'Learning Curves for Logistic Regression (F1)', fontsize=16)\n",
        "        plt.xlabel('Training Set Size', fontsize=14)\n",
        "        plt.ylabel('F1', fontsize=14)\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    n=4\n",
        "\n",
        "    print(f'Training score (F1):', train_scores_mean[-n])\n",
        "    print(f'Test score (F1):', test_scores_mean[-n])\n",
        "    print('Percent difference, train vs test:', - (train_scores_mean[-n] - test_scores_mean[-n]) / test_scores_mean[-n] * 100, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd39c61a262a72b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:55:58.195798Z",
          "start_time": "2025-02-05T14:55:57.800272Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "6cd39c61a262a72b",
        "outputId": "74507aa1-06c4-4db5-8b25-abaa86591b1a"
      },
      "outputs": [],
      "source": [
        "# Por defecto sklearn regulariza las regresiones logisticas usando `l2` por eso para ver los resultados sin regularizacion debemos usar penalty=None\n",
        "logistic_learning_curve(LogisticRegression(penalty=None), X_dep_train_prepared, y_dep_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e41e9f27947731e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:56:01.103255Z",
          "start_time": "2025-02-05T14:56:00.829341Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e41e9f27947731e",
        "outputId": "4284b80b-49aa-422c-d6a9-9641c2858b6d"
      },
      "outputs": [],
      "source": [
        "logistic_learning_curve(LogisticRegression(penalty='l2', C=1.0), X_dep_train_prepared, y_dep_train, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490d3a54f6bbf232",
      "metadata": {
        "id": "490d3a54f6bbf232"
      },
      "source": [
        "### Grid search y Selección de modelo\n",
        "\n",
        "Estas estrategias han sido útiles, pero ahora hemos introducido un nuevo problema. Cómo escogemos qué hyper-parámetros usar. Qué tipo de regularización usar? ¿Debemos probar todas estas combinaciones a mano?\n",
        "\n",
        "No. Sklearn nos provee con una herramienta para hacer la búsqueda del mejor modelo sobre los hyperparameters. Esta es `Grid Search CV`, pero para esto necesitamos una métrica para comparar los modelos. Esta no tiene por qué ser la perdida del modelo. En nuestro caso escogeremos el f1 score, ya que balancea la sensibilidad y especificidad del modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff021941496f3c0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-05T14:56:19.675842Z",
          "start_time": "2025-02-05T14:56:03.435439Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff021941496f3c0",
        "outputId": "5a830162-c5ff-49bd-f7ba-cf124ad65583"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Grid de parámetros\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# Inicializar el modelo de regresión logística\n",
        "log_reg = LogisticRegression(solver='saga', max_iter=10000)\n",
        "\n",
        "# Hacer la búsqueda\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='f1')\n",
        "grid_search.fit(X_dep_train_prepared, y_dep_train)\n",
        "\n",
        "# Obtener el mejor modelo y parámetros\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluar el mejor modelo en el conjunto de prueba\n",
        "y_pred = best_model.predict(X_dep_test_prepared)\n",
        "accuracy = accuracy_score(y_dep_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
